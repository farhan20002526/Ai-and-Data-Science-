{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a4f3da",
   "metadata": {},
   "source": [
    "# install beautifulsoap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d15909c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'urlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9236\\1974237238.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0murlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'urlib'"
     ]
    }
   ],
   "source": [
    "import urlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a8a36ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\data_science\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\data_science\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187424f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c92c7bb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'html_doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9236\\1962461628.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml_doc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprettify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'html_doc' is not defined"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e71020",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c56528",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9750465",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.p # it will get the first ocurence of the doc of html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da30a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.p[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e80100",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a # getting the first href from the web "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b4fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "links=soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d18db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.a['href'])\n",
    "print(soup.p['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aaa367",
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd787000",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in links:\n",
    "    print(l['class'],l['href'],l['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_p=soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0439d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in all_p:\n",
    "    print(p['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bdd4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(id=\"link1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0184f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all(\"a\"):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ccec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in soup.find_all('p'):\n",
    "    print(para.get(\"class\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4158f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e523440",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a41533",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pp in soup.find_all('p'):\n",
    "    print(pp.get_text())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f770925",
   "metadata": {},
   "outputs": [],
   "source": [
    "po = soup.find_all('p')\n",
    "po = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020fe051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d8300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "text=urllib.request.urlopen(\"https://www.w3schools.com/html/\")\n",
    "t=text.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbdcc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=BeautifulSoup(t,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92cdd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d490140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b250063",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a379dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "ko=urllib.request.urlopen(\"https://www.w3schools.com/\")\n",
    "y=ko.read()\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f345074",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui=BeautifulSoup(y,\"html.parser\")\n",
    "print(ui.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597fcab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=ui.find_all('p')\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31238a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in p:\n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef41dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=ui.find_all(\"a\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdda7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in a:\n",
    "    print(k.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ddf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "o=ui.find('class'=='w3-row w3-padding-32 ws-black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(o.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78d065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_soup = BeautifulSoup('<p id=\"my id\"></p>', 'html.parser')\n",
    "id_soup.p['id']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb78da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_soup.p.get_attribute_list('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f1d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = BeautifulSoup('<b id=\"boldest\">bold</b>', 'html.parser').b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68dafe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag['id']=\"verybold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585f5ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag.string.replace_with(\"Pakistan Zindabad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eaf31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "016842f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 're' has no attribute 'join'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9236\\2242445765.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlinks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mL\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"href\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mT\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Data_Science\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36mget_text\u001b[1;34m(self, separator, strip, types)\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m         return separator.join([s for s in self._all_strings(\n\u001b[0m\u001b[0;32m    294\u001b[0m                     strip, types=types)])\n\u001b[0;32m    295\u001b[0m     \u001b[0mgetText\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 're' has no attribute 'join'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Step 1: Send a GET request to the website\n",
    "\n",
    "url = \"https://thatsthem.com/name/Han-A-Tin\"  # Replace with the URL of the website you want to scrape\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Now you can process the response as needed\n",
    "\n",
    "# Step 2: Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Step 3: Extract the data you need from the parsed HTML\n",
    "# Let's say we want to extract all the links on the page\n",
    "links = soup.find_all(\"a\")\n",
    "text = soup.get_text()\n",
    "\n",
    "    # Extract phone numbers using regex pattern\n",
    "phone_pattern = r'\\d{3}-\\d{3}-\\d{4}'  # Matches the pattern ###-###-####\n",
    "phone_numbers = re.findall(phone_pattern, text)\n",
    "for link in links:\n",
    "    L=link.get(\"href\")\n",
    "    T=link.get_text(re)\n",
    "    print(T)\n",
    "    \n",
    "for number in phone_numbers:\n",
    "    print(number)\n",
    "\n",
    "# Step 4: Process and use the extracted data as per your requirement\n",
    "# For example, you can save the links to a file or perform further analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a2b97f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "559-781-9242\n",
      "719-322-7032\n",
      "641-533-4255\n",
      "559-781-9242\n",
      "719-322-7032\n",
      "641-533-4255\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "url = \"https://thatsthem.com/name/Tina-A-A-Chapman\"  # Replace with the URL of the website you want to scrape\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all the elements that may contain phone numbers\n",
    "phone_elements = soup.find_all(text=re.compile(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b'))\n",
    "\n",
    "# Extract the phone numbers from the elements\n",
    "phone_numbers = []\n",
    "for element in phone_elements:\n",
    "    phone_numbers.extend(re.findall(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', element))\n",
    "\n",
    "# Print the extracted phone numbers\n",
    "for phone_number in phone_numbers:\n",
    "    print(phone_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3409644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "559-781-9242\n",
      "719-322-7032\n",
      "641-533-4255\n",
      "559-781-9242\n",
      "719-322-7032\n",
      "641-533-4255\n",
      "730-740-2905\n",
      "730-740-2905\n",
      "214-490-7656\n",
      "214-490-7656\n",
      "972-564-1761\n",
      "256-467-3167\n",
      "215-954-7008\n",
      "910-424-6888\n",
      "336-788-3340\n",
      "214-490-7656\n",
      "214-490-7656\n",
      "972-564-1761\n",
      "256-467-3167\n",
      "215-954-7008\n",
      "910-424-6888\n",
      "336-788-3340\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>214-490-7656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>214-490-7656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>972-564-1761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256-467-3167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>215-954-7008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>910-424-6888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>336-788-3340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>214-490-7656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>214-490-7656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>972-564-1761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>256-467-3167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>215-954-7008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>910-424-6888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>336-788-3340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Phone\n",
       "0   214-490-7656\n",
       "1   214-490-7656\n",
       "2   972-564-1761\n",
       "3   256-467-3167\n",
       "4   215-954-7008\n",
       "5   910-424-6888\n",
       "6   336-788-3340\n",
       "7   214-490-7656\n",
       "8   214-490-7656\n",
       "9   972-564-1761\n",
       "10  256-467-3167\n",
       "11  215-954-7008\n",
       "12  910-424-6888\n",
       "13  336-788-3340"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "base_url = \"https://thatsthem.com/name/Tina-A-A-Chapman?page=\"  # Replace with the base URL of the website\n",
    "\n",
    "# Create a session object\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "})\n",
    "\n",
    "# Scrape multiple pages\n",
    "for page_number in range(1, 10):  # Replace with the range of page numbers you want to scrape\n",
    "    url = base_url + str(page_number)\n",
    "    response = session.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find all the elements that may contain phone numbers\n",
    "    phone_elements = soup.find_all(text=re.compile(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b'))\n",
    "\n",
    "    # Extract the phone numbers from the elements\n",
    "    phone_numbers = []\n",
    "    for element in phone_elements:\n",
    "        phone_numbers.extend(re.findall(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', element))\n",
    "\n",
    "    # Print the extracted phone numbers\n",
    "    for phone_number in phone_numbers:\n",
    "        print(phone_number)\n",
    "        data={\"Phone\": phone_numbers}\n",
    "\n",
    "        \n",
    "df=pd.DataFrame(data)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "676a52fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Send a GET request to the website\n",
    "\n",
    "url = \"https://thatsthem.com/name/Han-A-Tin\"  # Replace with the URL of the website you want to scrape\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Now you can process the response as needed\n",
    "\n",
    "# Step 2: Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Step 3: Extract the data you need from the parsed HTML\n",
    "# Let's say we want to extract all the links on the page\n",
    "links = soup.find_all(\"a\")\n",
    "text = soup.get_text()\n",
    "\n",
    "# Extract phone numbers using regex pattern\n",
    "phone_pattern = r'\\d{3}-\\d{3}-\\d{4}'  # Matches the pattern ###-###-####\n",
    "phone_numbers = re.findall(phone_pattern, text)\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "data = {'Link': [], 'Phone Number': [], 'Text': []}\n",
    "\n",
    "# Populate the data dictionary\n",
    "for link in links:\n",
    "    L = link.get(\"href\")\n",
    "    T = link.get_text()\n",
    "    data['Link'].append(L)\n",
    "    data['Text'].append(T)\n",
    "\n",
    "# Pad the shorter lists with None values using zip()\n",
    "max_length = max(len(data['Link']), len(phone_numbers), len(data['Text']))\n",
    "data['Phone Number'] += phone_numbers + [None] * (max_length - len(phone_numbers))\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "\n",
    "# Convert DataFrame to Excel file\n",
    "excel_filename = \"web_data.xlsx\"  # Specify the desired filename\n",
    "df.to_excel(excel_filename, index=False)\n",
    "\n",
    "print(\"Excel file saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "006f38a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['214-490-7656',\n",
       " '214-490-7656',\n",
       " '972-564-1761',\n",
       " '256-467-3167',\n",
       " '215-954-7008',\n",
       " '910-424-6888',\n",
       " '336-788-3340',\n",
       " '214-490-7656',\n",
       " '214-490-7656',\n",
       " '972-564-1761',\n",
       " '256-467-3167',\n",
       " '215-954-7008',\n",
       " '910-424-6888',\n",
       " '336-788-3340']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e321ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_pages(num_pages):\n",
    "    # Define the base URL and the number of pages to scrape\n",
    "    base_url = \"https://thatsthem.com/name/Han-A-Tin?page=\"\n",
    "    \n",
    "    # Create a list to store the data from each page\n",
    "    all_data = []\n",
    "    \n",
    "    # Step 1: Send a GET request to each page and scrape the data\n",
    "    for page in range(1, num_pages + 1):\n",
    "        url = base_url + str(page)\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        links = soup.find_all(\"a\")\n",
    "        text = soup.get_text()\n",
    "        phone_pattern = r'\\d{3}-\\d{3}-\\d{4}'  # Matches the pattern ###-###-####\n",
    "        phone_numbers = re.findall(phone_pattern, text)\n",
    "    \n",
    "        # Create a dictionary to store the data from the current page\n",
    "        data = {'Link': [], 'Phone Number': [], 'Text': []}\n",
    "    \n",
    "        for link in links:\n",
    "            L = link.get(\"href\")\n",
    "            T = link.get_text()\n",
    "            data['Link'].append(L)\n",
    "            data['Text'].append(T)\n",
    "    \n",
    "        # Pad the shorter lists with None values using zip()\n",
    "        max_length = max(len(data['Link']), len(phone_numbers), len(data['Text']))\n",
    "        data['Phone Number'] += phone_numbers + [None] * (max_length - len(phone_numbers))\n",
    "    \n",
    "        # Append the data from the current page to the all_data list\n",
    "        all_data.append(data)\n",
    "    \n",
    "    # Step 2: Create a DataFrame from the collected data\n",
    "    df = pd.concat([pd.DataFrame(data) for data in all_data], ignore_index=True)\n",
    "    \n",
    "    # Step 3: Convert DataFrame to Excel file\n",
    "    excel_filename = \"Data.xlsx\"  # Specify the desired filename\n",
    "    df.to_excel(excel_filename, index=False)\n",
    "    \n",
    "    print(\"Excel file saved successfully.\")\n",
    "\n",
    "# Call the function and specify the number of pages to scrape\n",
    "scrape_pages(num_pages=3)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab2984c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
